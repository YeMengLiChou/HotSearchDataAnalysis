# 调试用
[debug]
#enable = true
enable = false

# ======================== redis配置 ==========================
[redis]
host = "127.0.0.1"
port = 6379
db = 0

[kafka]
# host = "172.27.237.77"  # wsl
host = "172.17.160.199" # aliyun
port = 9092

# ====================== scrapy ======================
[scrapy]
# settings.py 在当前项目下的路径
settings_path = "scraper.scraper.settings"
# 要启动爬虫的名称name
spider_name = "hot search"
# 爬虫启动间隔，单位 min
scrapy_start_interval = 10


[scrapy.settings]  # for settings.py
# 并发请求数
CONCURRENT_REQUESTS = 6
# 请求间隔
DOWNLOAD_DELAY = 0
# 请求超时
DOWNLOAD_TIMEOUT = 10
# item并发数
CONCURRENT_ITEMS = 64
# 请求并发数
CONCURRENT_REQUESTS_PER_DOMAIN = 16
# 重试次数
RETRY_TIMES = 3
# 最大线程池大小
REACTOR_THREADPOOL_MAXSIZE = 32
# 自动调节
AUTOTHROTTLE_ENABLED = false
# 下载延迟
AUTOTHROTTLE_START_DELAY = 2
# 最大延迟
AUTOTHROTTLE_MAX_DELAY = 10

# 启用重定向
REDIRECT_ENABLED = true

[scrapy.kafka]
# 爬取的item发送的主题
topic = "hot_1"
key = "a"

[scrapy.log]
LOG_LEVEL = "WARNING"
LOG_FILE = "logs/scrape.log"
LOG_FILE_BACKUP_COUNT = 100
LOG_FILE_TYPE="size"
# ============== spark ================


[spark]
# spark 读取kafka的主题
kafka.topic = "hot_1"
